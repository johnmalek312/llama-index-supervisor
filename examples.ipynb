{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for logging, ignore this cell if you are not using arize\n",
    "import llama_index.core\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates a basic supervisor workflow.\n",
    "\n",
    "1.  **Define Tools**: Functions for `add`, `multiply`, and `web_search` are defined and converted into `FunctionTool` objects.\n",
    "2.  **Create Agents**: Two specialized `FunctionAgent` instances are created:\n",
    "    *   `math_agent`: Uses `add_tool` and `multiply_tool`.\n",
    "    *   `research_agent`: Uses `search_tool`.\n",
    "3.  **Initialize Supervisor**: A `Supervisor` is created to manage the `math_agent` and `research_agent`. The `add_tree_structure=True` adds json context to the llm about the hierarchy of agents and tools.\n",
    "4.  **Run Workflow**: The supervisor is executed with the input query \"what's the combined headcount of the FAANG companies in 2024?\". The supervisor delegates the task to the appropriate agent(s) based on the query and tool capabilities. First, the `research_agent` is likely called to find the headcounts, and then the `math_agent` is called to sum them up.\n",
    "5.  **Print Response**: The final response from the supervisor is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The combined headcount of the FAANG companies in 2024 is 1,977,586 employees.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index_supervisor import Supervisor\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define tools\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return (\n",
    "        \"Here are the headcounts for each of the FAANG companies in 2024:\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 employees.\\n\"\n",
    "        \"2. **Apple**: 164,000 employees.\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 employees.\\n\"\n",
    "        \"4. **Netflix**: 14,000 employees.\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 employees.\"\n",
    "    )\n",
    "\n",
    "# Create function tools\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "search_tool = FunctionTool.from_defaults(fn=web_search)\n",
    "\n",
    "# Create specialized agents\n",
    "math_agent = FunctionAgent(\n",
    "    name=\"math_expert\",\n",
    "    llm=llm,\n",
    "    tools=[add_tool, multiply_tool],\n",
    "    system_prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"research_expert\",\n",
    "    llm=llm,\n",
    "    tools=[search_tool],\n",
    "    system_prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# Create supervisor workflow\n",
    "supervisor = Supervisor(\n",
    "    llm=llm,\n",
    "    agents=[math_agent, research_agent],\n",
    "    add_tree_structure=True,\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "# Run the workflow\n",
    "ctx = Context(supervisor)\n",
    "response = await supervisor.run(\n",
    "    input=\"what's the combined headcount of the FAANG companies in 2024?\",\n",
    "    ctx=ctx\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example showcases a hierarchical supervisor structure:\n",
    "\n",
    "1.  **Define Tools**: Functions for `add`, `multiply`, `web_search`, `format_document`, and `publish_content` are defined and converted into `FunctionTool` objects.\n",
    "2.  **Create Base Agents**: Four specialized `FunctionAgent` instances are created: `math_agent`, `research_agent`, `writing_agent`, and `publishing_agent`, each equipped with relevant tools.\n",
    "3.  **Create Mid-Level Supervisors**: Two `Supervisor` instances are created:\n",
    "    *   `research_supervisor`: Manages `research_agent` and `math_agent`.\n",
    "    *   `writing_supervisor`: Manages `writing_agent` and `publishing_agent`.\n",
    "4.  **Create Top-Level Supervisor**: A `top_level_supervisor` is created to manage the `research_supervisor` and `writing_supervisor`, establishing a three-tier hierarchy. The `add_tree_structure=True` adds json context to the llm about the hierarchy of agents and tools.\n",
    "5.  **Run Workflow**: The `top_level_supervisor` is executed with the input query \"what's the combined headcount of the FAANG companies in 2024?\". The top supervisor delegates the task to the appropriate mid-level supervisor (`research_supervisor`), which in turn delegates to the appropriate base agent(s) (`research_agent` then `math_agent`).\n",
    "6.  **Print Response**: The final response coordinated through the hierarchy is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The research expert was unable to find explicit headcount numbers for the FAANG companies in 2024 through web searches. It is recommended to check the latest annual reports or press releases from each company for the most accurate and up-to-date information. If you need further assistance or a different approach, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index_supervisor import Supervisor\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define all the required tools\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return \"Web search results for: \" + query\n",
    "\n",
    "def format_document(content: str) -> str:\n",
    "    \"\"\"Format document with proper structure and styling.\"\"\"\n",
    "    return f\"Formatted document: {content}\"\n",
    "\n",
    "def publish_content(content: str) -> str:\n",
    "    \"\"\"Publish content to appropriate channels.\"\"\"\n",
    "    return f\"Published: {content}\"\n",
    "\n",
    "# Create function tools\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "search_tool = FunctionTool.from_defaults(fn=web_search)\n",
    "format_tool = FunctionTool.from_defaults(fn=format_document)\n",
    "publish_tool = FunctionTool.from_defaults(fn=publish_content)\n",
    "\n",
    "# Create base-level agents\n",
    "math_agent = FunctionAgent(\n",
    "    name=\"math_expert\",\n",
    "    llm=llm,\n",
    "    tools=[add_tool, multiply_tool],\n",
    "    system_prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"research_expert\", \n",
    "    llm=llm,\n",
    "    tools=[search_tool],\n",
    "    system_prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "writing_agent = FunctionAgent(\n",
    "    name=\"writing_expert\",\n",
    "    llm=llm,\n",
    "    tools=[format_tool],\n",
    "    system_prompt=\"You are a professional writer who formats and improves content.\"\n",
    ")\n",
    "\n",
    "publishing_agent = FunctionAgent(\n",
    "    name=\"publishing_expert\",\n",
    "    llm=llm,\n",
    "    tools=[publish_tool],\n",
    "    system_prompt=\"You are a publishing expert who knows how to distribute content effectively.\"\n",
    ")\n",
    "\n",
    "# Create mid-level supervisors (research team and writing team)\n",
    "research_supervisor = Supervisor(\n",
    "    llm=llm,\n",
    "    agents=[research_agent, math_agent],\n",
    "    name=\"research_supervisor\",\n",
    "    system_prompt=\"You manage a research team with experts in research and math. Delegate tasks appropriately.\",   \n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "writing_supervisor = Supervisor(\n",
    "    llm=llm,\n",
    "    agents=[writing_agent, publishing_agent],\n",
    "    name=\"writing_supervisor\", \n",
    "    system_prompt=\"You manage a content team with experts in writing and publishing. Delegate tasks appropriately.\",\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "# Create top-level supervisor\n",
    "top_level_supervisor = Supervisor(\n",
    "    llm=llm,\n",
    "    agents=[research_supervisor, writing_supervisor],\n",
    "    name=\"top_level_supervisor\",\n",
    "    system_prompt=\"You are the executive supervisor coordinating between the research and writing teams. For research or math problems, use the research_supervisor. For content creation and publishing, use the writing_supervisor.\",\n",
    "    timeout=60,\n",
    "    add_tree_structure=True\n",
    ")\n",
    "\n",
    "query = \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "\n",
    "ctx = Context(top_level_supervisor)\n",
    "response = await top_level_supervisor.run(\n",
    "    input=query,\n",
    "    ctx=ctx,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The total number of employees across all FAANG companies in 2024 is 1,000,000.')]), raw=ChatCompletionChunk(id='chatcmpl-BJKyiHSvUAIe43ZbrzFn0twZebwKk', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1743948572, model='gpt-4o-2024-08-06', object='chat.completion.chunk', service_tier='default', system_fingerprint='fp_898ac29719', usage=None), delta='', logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adding chat messages\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "from llama_index.core.llms.llm import ChatMessage\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index_supervisor import Supervisor\n",
    "\n",
    "ctx = Context(\n",
    "    supervisor\n",
    ")\n",
    "memory=ChatMemoryBuffer.from_defaults(\n",
    "    chat_history=[\n",
    "        # these messages will be added to the chat history right after the system prompt\n",
    "        ChatMessage(role=\"user\", content=\"what's the combined headcount of the FAANG companies in 2024?\"),\n",
    "        ChatMessage(role=\"assistant\", content=\"The combined headcount is 1,000,000.\"),\n",
    "    ]\n",
    ")\n",
    "await ctx.set(\"memory\", memory)\n",
    "\n",
    "supervisor = Supervisor(llm=llm, agents=[math_agent, research_agent], system_prompt=\"You are a math expert and a world class researcher with access to web search.\")\n",
    "await supervisor.run(\n",
    "    input=\"Paraphrase your answer.\",\n",
    "    ctx=ctx,\n",
    ")\n",
    "\n",
    "# chat history will look like this\n",
    "# [\n",
    "#     ChatMessage(role=\"system\", content=\"You are a math expert and a world class researcher with access to web search.\"),\n",
    "#     ChatMessage(role=\"user\", content=\"what's the combined headcount of the FAANG companies in 2024?\"),\n",
    "#     ChatMessage(role=\"assistant\", content=\"The combined headcount is 1,000,000.\"),\n",
    "#     ChatMessage(role=\"user\", content=\"Paraphrase your answer.\")\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "from llama_index.core.llms.llm import ChatMessage\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index_supervisor import Supervisor\n",
    "\n",
    "### you can also use your own workflow as an agent and pass it into the supervisor\n",
    "from llama_index.core.workflow import Workflow, Context, StartEvent, StopEvent, step\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "class CoolAgent(Workflow):\n",
    "    def __init__(self, name: str, description: str, llm: LLM, system_prompt: str = \"You are a cool agent. Be cool.\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.llm = llm\n",
    "        self.system_message = ChatMessage(role=\"system\", content=system_prompt)\n",
    "    @step\n",
    "    async def start_flow(self, ctx: Context, ev: StartEvent) -> StopEvent:\n",
    "        # the supervisor send a copy of the chat history through the ctx\n",
    "        memory: ChatMemoryBuffer = await ctx.get(\"memory\", default=ChatMemoryBuffer.from_defaults(llm=self.llm))\n",
    "        # you can use the memory to get the chat history\n",
    "        # messages = memory.get() # or get_all()\n",
    "        # do something with the messages\n",
    "        await memory.aput(\n",
    "            ChatMessage(role=\"user\", content=\"How are you?\")\n",
    "        )\n",
    "        response = await self.llm.achat([self.system_message] + memory.get()) # dont add the system message to the memory so it doesnt get sent back to the supervisor\n",
    "        \n",
    "        # add response to the memory\n",
    "        await memory.aput(response.message)\n",
    "        # set ctx(\"memory\") to the updated memory (just in case no memory key in context was passed in the first place)\n",
    "        await ctx.set(\"memory\", memory)\n",
    "        return StopEvent(result=response.message) # the supervisor doesn't check the result of the workflow, so you can return anything you want in case you use the agent\n",
    "    \n",
    "\n",
    "cool_agent = CoolAgent(\n",
    "    name=\"cool_agent\",\n",
    "    description=\"A cool agent that does cool things.\",\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a cool agent. Be super super cool.\"\n",
    ")\n",
    "\n",
    "supervisor = Supervisor(\n",
    "    llm=llm,\n",
    "    agents=[cool_agent]\n",
    ")\n",
    "\n",
    "result = await supervisor.run(\n",
    "    input=\"Ask the cool agent how it is doing.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The cool agent says it's \"doing as cool as a cucumber in a bowl of hot sauce!\" How about you?\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
